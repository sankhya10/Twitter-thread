{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhbmR2oTxAdY",
        "outputId": "c2afc03b-79ce-4216-bc81-aeea6077f728"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import html\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#if there is a problem in importing transformer\n",
        "# please install using pip from the code below\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os.path\n",
        "from dateutil import parser\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYJOwRX-xF__"
      },
      "source": [
        "#!pip install trafilatura"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsDompMrxGhj",
        "outputId": "b7e194bd-82e8-4e40-9798-adaa698bf1bd"
      },
      "source": [
        "import trafilatura\n",
        "downloaded = trafilatura.fetch_url('https://www.nginx.com/blog/now-worlds-1-web-server-nginx-looks-forward-to-even-brighter-future/?utm_medium=email&utm_source=nginxdb&utm_campaign=ww-nx_mad&utm_content=bg&mkt_tok=NjUzLVNNQy03ODMAAAF-ItKyIbnEbPZ46g8i-GcuhvIcWazwBd-R9JPPX4bEN86nCcd-MGGs_n6sdIKJxFPlCMDPfZqKqlwSe6e_543i9JJJO6t1VITuU24-JIsNBDIbqgm7')\n",
        "trafilatura.extract(downloaded)\n",
        "# outputs main content and comments as plain text ...\n",
        "from trafilatura import bare_extraction\n",
        "#bare_extraction(downloaded)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'author': 'Tony Mauro',\n",
              " 'categories': ['Blog,News'],\n",
              " 'comments': '',\n",
              " 'date': '2021-06-10',\n",
              " 'description': \"NGINX recently became the most popular web server in the world, according to W3Techs. We're profoundly grateful to the NGINX community, who've brought us to this milestone, and look forward to providing even more tools to help you optimize delivery of your modern and cloud-native apps.\",\n",
              " 'fingerprint': None,\n",
              " 'hostname': 'nginx.com',\n",
              " 'id': None,\n",
              " 'license': None,\n",
              " 'sitename': 'NGINX',\n",
              " 'tags': [],\n",
              " 'text': 'Last month, NGINX passed a major milestone. W3Techs announced that after many years of steady growth in market share, NGINX is now the most popular web server in the world, edging out Apache HTTP Server. Of course we’re thrilled to be recognized for our years of innovation and hard work, but even more than that we’re profoundly grateful to you, the members of the NGINX community, who’ve brought us to this milestone by trusting us to secure and deliver your websites and apps.\\nThough W3Techs measures NGINX usage specifically for web serving, over NGINX’s nearly 20-year history we’ve added many more capabilities – reverse proxying, load balancing, traffic shaping, caching, and security controls, among others – and you’ve embraced them enthusiastically. It’s fair to say NGINX has become a Swiss Army Knife™ for developers and platform ops teams.\\nBut by no means do we consider reaching this milestone a reason to stop and rest. It’s a booster shot, energizing us to come up with even more tools you can use to deliver the outstanding digital experiences your users expect.\\nThe History and Evolution of NGINX\\nThe original motivation for creating NGINX wasn’t nearly so grand. Back in 2001, NGINX’s original creator Igor Sysoev was trying to solve a problem at work – his web servers were having trouble keeping up with ever‑increasing numbers of requests. The challenge was referred to at the time as the C10K problem – handling 10,000 simultaneous client connections to clients.\\nInspired by the design of Unix and other classic distributed systems, Igor developed an event‑driven architecture that is so lightweight, scalable, and powerful it’s still at the heart of NGINX today. Knowing he was onto something with the potential to help other websites, he open sourced NGINX in 2004.\\nAs detailed in this infographic, community contributions and adoption continued apace, leading to the founding of NGINX, Inc. in 2011. Fall 2014 saw the debut of NGINX Plus, built on NGINX Open Source and enhanced with production‑grade features and commercial support for enterprise customers.\\nRevenues from NGINX Plus enabled us to expand our staff over the following years and continue development of both the open source and commercial models. In 2019 F5, the industry’s leading vendor of hardware‑based security appliances and application delivery controllers (ADCs), acquired NGINX to spearhead the company’s expansion into modern and cloud‑native application environments.\\nAs adoption of microservices, APIs, and Kubernetes has accelerated over the past few years, our community and customers have called on us for help in leveraging the new technologies and taming the complexity that can result. We’ve responded with tools like NGINX Unit for dynamic, polyglot application serving, and NGINX Ingress Controller and NGINX Service Mesh for secured management of traffic into and within Kubernetes‑based container environments. Our future lies firmly in this world of container orchestration and management and delivery of cloud‑native applications.\\nA Modular Architecture Has Served Us Well\\nNGINX’s event‑driven, modular, and highly parallelized architecture initially enabled developers and website owners to move more packets, more quickly and with greater flexibility, on the servers they already had.\\nThe NGINX community has consistently recognized the broad possibilities of this design – sometimes ahead of us at NGINX. They quickly realized NGINX can do a lot more than serve web pages. Without any prompting or documentation or guidance, novel use cases emerged. Enthusiastic users developed hundreds of modules to extend NGINX functionality (more than 150 and counting). Some in the community started using NGINX as an API gateway. Others linked NGINX servers together for load balancing of Apache and NGINX web servers. Many saw NGINX Open Source as an ideal Ingress controller for Kubernetes, a use case so promising we built and open sourced our own Ingress Controller.\\nBuilding Roadmap and a Future With Community Help\\nWe took all of this community activity as invaluable guidance about the additional capabilities our users wanted most. It’s no exaggeration to say that the NGINX community has been by far our best source of inspiration for product innovation. We’ve plowed the revenues from our continually growing commercial customer base back into ongoing development of commercial and open source offerings alike.\\nOver the years we have also watched our community evolve. We started out serving startups in the early days of cloud computing. As the community that’s developing and deploying modern and cloud‑native apps has grown, we’ve grown along with it and are now a trusted partner with all of the major cloud vendors and some of the world’s largest enterprises across many industries. The NGINX community today ranges from one‑person startups to the world’s largest technology companies like Adobe.\\nMore Mature, But Still Scrappy\\nWhat we love is that even within those large companies, we often see developers using our open source software in their personal sandboxes while the corporate security and platform ops teams rely on our commercial products – NGINX Plus, NGINX Controller, and NGINX App Protect, among others – to deliver mission‑critical SaaS products and complicated modern web apps with millions of paying customers.\\nNGINX Open Source always has been and remains the core of our product offerings, and we are proud and happy that so many developers trust and rely on it. At the same time, our commercial products with augmented scalability, security, and management features deliver the higher‑end capabilities that enterprises need.\\nEffectively meeting the needs and wants of our open source community while also growing our revenues and building compelling enterprise products is a constant balancing act. I joined NGINX a number of years ago when the company had fewer than 100 people. Today the NGINX Product Group at F5 stands at over 250 people and growing, with hundreds more supporting the product group across business functions in F5. Even as our market share, product line, and user base has grown, one of the things I love is that NGINX remains the same scrappy upstart at heart. We don’t take our success for granted, and we love doing the little things that make the big things possible.\\nWe value our community just as much today as we did a decade ago. It is the source of our strength as a company and our path to future growth. Thanks for making us the #1 server of websites and apps, and stay tuned as we roll out more products and possibilities in the near future.\\nWant to try out our enterprise‑grade solutions for yourself? Free 30-day trials are available for all of them:\\n- NGINX Plus and NGINX App Protect\\n- NGINX Ingress Controller and NGINX App Protect\\n- NGINX Controller\\n- NGINX Instance Manager\\n- F5 DNS Load Balancer Cloud Service and F5 Secondary DNS Cloud Service\\nOr get started with free and open source offerings:',\n",
              " 'title': \"Now the World's #1 Web Server, NGINX Looks Forward to an Even Brighter Future - NGINX\",\n",
              " 'url': 'https://www.nginx.com/blog/now-worlds-1-web-server-nginx-looks-forward-to-even-brighter-future/'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKU33mt3xWqP",
        "outputId": "0fe9292b-88e7-4f93-b31e-79980ef95b74"
      },
      "source": [
        "#Calculating the number of questions as needed depending upon the length of text\n",
        "import math\n",
        "mohack = []\n",
        "text = bare_extraction(downloaded)['text']\n",
        "mohack=text.splitlines()\n",
        "number_of_threads = math.trunc(len(text)/3200)\n",
        "number_of_threads"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPo0-OaN4Cfa",
        "outputId": "18d067b3-1430-4164-fea4-706d6fffdda0"
      },
      "source": [
        "#Installing a libraty to generate question using transformer n-to-n model\n",
        "!git clone https://github.com/amontgomerie/question_generator"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'question_generator' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjxzsXTE4wcF",
        "outputId": "74d39b82-7d53-4c8b-9f3b-695f72f58aa0"
      },
      "source": [
        "%cd question_generator/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/question_generator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bFWC1BN485o",
        "outputId": "77afb47f-9611-4ef5-b755-c32677996381"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34marticles\u001b[0m/  LICENSE       questiongenerator.py  requirements.txt  setup.py\n",
            "\u001b[01;34mexamples\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  README.md             run_qg.py         \u001b[01;34mtraining\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0IOUChvAr-t"
      },
      "source": [
        "# !pip install transformers\n",
        "# !pip install sentencepiece"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkOt3lMw5C-q"
      },
      "source": [
        "#importing the library for question generation\n",
        "from questiongenerator import QuestionGenerator\n",
        "qg = QuestionGenerator()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwxyfwRw_rG9",
        "outputId": "583f55f9-e174-4b41-fc27-4ed7702e43ab"
      },
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "assert device == torch.device('cuda')\n",
        "device"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27fKSqhG6OIW",
        "outputId": "09f4806f-c12f-471b-e44e-4e5b50b0f6a3"
      },
      "source": [
        "#Generating question and answer with respect to the length of the text\n",
        "#number of question would result number of threads\n",
        "lp = qg.generate(text, num_questions=number_of_threads,answer_style='all')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating questions...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating QA pairs...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-MaD1SFCK35",
        "outputId": "8136f581-a685-42a3-addc-ca3d551de3d8"
      },
      "source": [
        "#Now appending the questions in the text and then calculating cosine similarity with all \n",
        "#the paragraphs and then extracting those paragraphs to build summary\n",
        "for i in range(number_of_threads):\n",
        "  mohack.append(lp[i]['question'])\n",
        "\n",
        "import gensim\n",
        "# upgrade gensim if you can't import softcossim\n",
        "from gensim.matutils import softcossim \n",
        "from gensim import corpora\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "print(gensim.__version__)\n",
        "#> '3.6.0'\n",
        "\n",
        "# Download the FastText model#\n",
        "#\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R31Qmp29iUDd"
      },
      "source": [
        "#using fasttext model to calculate similarity\n",
        "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbSccs0OiUeI"
      },
      "source": [
        "# Prepare a dictionary and a corpus.\n",
        "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in mohack])\n",
        "\n",
        "# Prepare the similarity matrix\n",
        "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEQ3o1IHnDxH",
        "outputId": "7ee32668-0387-4542-8369-f8ba0c360ee7"
      },
      "source": [
        "#create bag of words model for all the paragraphs \n",
        "sentences = []\n",
        "for i in range(len(mohack)):\n",
        "  sentences.append(dictionary.doc2bow(simple_preprocess(mohack[i])))\n",
        "print(softcossim(sentences[0], sentences[1], similarity_matrix))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.573541026789918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jMbdw4joJCo"
      },
      "source": [
        "#we compute the cosim=ne similarity matrix between all the paragraphs and qiestions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_soft_cossim_matrix(sentences):\n",
        "    len_array = np.arange(len(sentences))\n",
        "    xx, yy = np.meshgrid(len_array, len_array)\n",
        "    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
        "    return cossim_mat\n",
        "\n",
        "matrix =create_soft_cossim_matrix(sentences)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPu7FgCbqqwF",
        "outputId": "c09de665-9cdf-4c1e-e810-4d66f7d56a2b"
      },
      "source": [
        "#now we will extract the paragraphs which are related to question and then \n",
        "#generate thier summary \n",
        "context = {}\n",
        "for i in range(1,number_of_threads + 1):\n",
        "  lm = []\n",
        "  for j in range(0,len(matrix.iloc[-i])-number_of_threads):\n",
        "    #only strong similarity connection paragraphs would be appended\n",
        "    if matrix.iloc[-i][j] >= 0.65 and matrix.iloc[-i][j] < 1.00 :\n",
        "      lm.append(j) \n",
        "    context[lp[-i]['question']] = lm \n",
        "context"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'What is the main reason why NGINX is the #1 server?': [],\n",
              " 'What was the motivation for NGINX to become the most popular web server in the world?': [0,\n",
              "  4,\n",
              "  5,\n",
              "  14,\n",
              "  18]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQxn-4nrPggh"
      },
      "source": [
        "from transformers import BartTokenizer, BartModel\n",
        "import torch\n",
        "#for the process of abstractive summmarization I have used Bart-cnn from hugging face as pretrained model\n",
        "from typing import List\n",
        "#bart-cnn uses encoder and decoder for summarization\n",
        "#but it's only effective for tokens less than 1300 around\n",
        "from transformers import BartForConditionalGeneration\n",
        "def old_summarization_pipeline(text: List[str]) -> List[str]:\n",
        "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "    input_ids = tokenizer.batch_encode_plus(text, truncation=True, padding=True, return_tensors='pt', max_length=1024)['input_ids']\n",
        "    summary_ids = model.generate(input_ids)\n",
        "    summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=False) for s in summary_ids]\n",
        "    return summaries"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzbIFzfatKza"
      },
      "source": [
        "#Now cleaning stuff annd then the last part i.e. summarization\n",
        "for i,j in context.items():\n",
        "  lm = [] \n",
        "  for k in range(len(j)):\n",
        "    lm.append(old_summarization_pipeline([mohack[j[k]]]))\n",
        "    context[i] = lm\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6RbvTUY0ODb"
      },
      "source": [
        "twitterthread = {}\n",
        "finalsummary = []\n",
        "for i,j in context.items():\n",
        " lmo = []\n",
        " for k in range(len(j)):\n",
        "    lmo = j[k][0].split(\".\")\n",
        "    new_list = [elem for elem in lmo if elem.strip()]\n",
        "    finalsummary.append(new_list)\n",
        "    dict1= {}\n",
        "    d = 0\n",
        "    for l in range(len(finalsummary)):\n",
        "      for m in range(len(finalsummary[l])):\n",
        "        d = d+1\n",
        "        dict1[d] = finalsummary[l][m]\n",
        "    twitterthread[i] = dict1 \n",
        "  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gruUXlKT0O88",
        "outputId": "cd96614c-348d-4aed-993d-c64ca8e30cac"
      },
      "source": [
        "twitterthread"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'What was the motivation for NGINX to become the most popular web server in the world?': {1: 'NGINX is now the most popular web server in the world, edging out Apache HTTP Server',\n",
              "  2: ' W3Techs announced that after many years of steady growth in market share, NGINX has overtaken Apache',\n",
              "  3: \" We're thrilled to be recognized for our years of innovation and hard work, but even more than that we’re profoundly grateful to you\",\n",
              "  4: \"The original motivation for creating NGINX wasn't nearly so grand\",\n",
              "  5: ' Igor Sysoev was trying to solve a problem at work – his web servers were having trouble keeping up with ever‑increasing numbers of requests',\n",
              "  6: ' The challenge was referred to at the time as the C10K problem – handling 10,000 simultaneous client connections',\n",
              "  7: ' NGINX is a lightweight, scalable, and powerful distributed system',\n",
              "  8: ' It was created by Igor Kuznetsov in 2004 and is still at the heart of many websites today',\n",
              "  9: ' Kuznetskov: \"Igor developed an event‑driven architecture that is so lightweight and scalable\"',\n",
              "  10: 'NGINX started out serving startups in the early days of cloud computing',\n",
              "  11: ' The NGINX community today ranges from one‑person startups to the world’s largest technology companies like Adobe',\n",
              "  12: \" We are now a trusted partner with all of the major cloud vendors and some of the world's largest enterprises\",\n",
              "  13: ' NGINX Product Group at F5 stands at over 250 people and growing',\n",
              "  14: ' Even as our market share, product line, and user base has grown, one of the things I love is that NGinX remains the same scrappy upstart at heart',\n",
              "  15: \" We don't take our success for granted, and we love doing the little things that make the big things possible\"}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5gQ6x746Jfc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}